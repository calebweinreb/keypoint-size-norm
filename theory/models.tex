\documentclass{article}         %% What type of document you're writing.
\usepackage[fleqn]{amsmath}
\usepackage{amsfonts,amssymb}   %% AMS mathematics macros
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amssymb}
\usepackage{parskip}
\usepackage{mathtools}
\usepackage{bm}
\usepackage[margin=1.3in]{geometry}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{tikz}
%\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}
\usetikzlibrary{backgrounds}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newcommand\descitem[1]{\item{\bfseries #1}}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\argmax}{\arg\max}
\DeclareMathOperator{\pad}{pad}
\graphicspath{ {./figs/} }
\newcommand{\inv}{^{-1}}
\newcommand{\pd}{\partial}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\cb}[1]{\left\{ #1 \right\}}
\newcommand{\pn}[1]{\left( #1 \right)}
\newcommand{\bc}[1]{\left[ #1 \right]}
\newcommand{\eps}{\varepsilon}
\DeclareMathOperator{\Tr}{Tr}
\usepackage{titling}

\title{Size norm model ideas}
\author{Caleb Weinreb and Kai Fox}
\date{May 2023}

\begin{document}

\maketitle

\section{Modeling framework}

Suppose we have pose data $\{y_t\}_{t=1}^T$ and $\{y'_t\}_{t=1}^{T'}$ for a pair of animals, where $y_t, y'_t \in \mathbb{R}^{KD}$, represent the positions of $K$ keypoints in $D$ dimensions. For now, we will assume that the keypoints are in egocentric coordinates, meaning each animal is always centered and pointing in the same direction. Let's also assume the data have been rescaled to account for gross size differences, so that all remaining differences are subtleties of body shape. Our goal is to define a canonical (low-dimensional) pose space and learn how the data from each animal map onto it. Put in terms of generative modeling, we wish to explain each animal's observed pose $y_t$ as the (noisy) realization of some latent pose state $x_t \in \mathbb{R}^M$, where the space of latent states is shared between animals. This can be formalized as follows:
%
\begin{align}
    y_t & = F(x_t) + \xi \ \text{where} \ 
    \xi \sim \mathcal{N}(0, R) \ \text{and} \ 
    x_t \sim P_x \\
    y_t' & = F'(x_t') + \xi' \ \text{where} \ 
    \xi' \sim \mathcal{N}(0, R') \ \text{and} \ 
    x_t' \sim P_x'
\end{align}
%
where $F, F'$ are respective functions mapping from the latent space to each animal's pose space, and $P_x, P_x'$ are distributions over latent states. Ideally, we want $F$ and $F'$ to capture morphological differences between the two animals, and $P_x, P_x'$ to capture differences in the frequency of behaviors. We also want to make sure that $F$ and $F'$ are as similar as possible, i.e. to avoid arbitrary rotations of the latent space for one animal compared to the other. Here are some ideas for how to achieve these goals:
\begin{itemize}
    \item \textbf{Shared Gaussian pose distribution:} We could start with a simple shared distribution over the latent space $P_x = P_x' = \mathcal{N}(0, I_M)$. 
    \item  \textbf{Gaussian mixture pose distribution:} Another option is to model $P_x$ and $P_x'$ as Gaussian mixtures, where the mixture components are shared between animals, but the mixture weights are allowed to vary. This would naturally capture differences in the frequency of behaviors between animals.
    \item \textbf{Affine pose mappings:} We probably want to assume that $F$ and $F'$ are affine, i.e. that $F(z) = Az + b$ for some $A \in \mathbb{R}^{KD \times M}$ and $b \in \mathbb{R}^{KD}$. 
    \item \textbf{Ensuring similar mappings:} There are a few ways to make sure that $F$ and $F'$ are similar. One is to model their parameters as additive perturbations of a common mapping, e.g. $F = (A + \Delta A, b + \Delta b)$, $F' = (A + \Delta A', b + \Delta b')$ where $A,b$ can vary broadly but $\Delta A, \Delta b, \Delta A', \Delta b'$ have a tight prior. Another option is to perturb the common mapping multiplicatively.
\end{itemize}

A core simplifying assumption of all of our pose space models $P_z$ will be independence at each time step. In this way our model is a deepening of keypoint-moseq's affine transform from pose space to keypoint space $y \sim \NN(Cx+d, R)$, where we vary the transform per-mouse and investigate other transforms, while ignoring all dynamical character of behavior. 


\subsection{Pared-down Gaussian mixture model}
\label{sec:pared-down-gmm}

Before deriving EM steps for our full desired context, we begin with a pared-down model for only one animal, where we take $F = \mathrm{Id}$ and $R = \eps I$
\begin{align}
    z_t & \sim \text{Cat}(\pi) &
    x_t & \sim \mathcal{N}(m_{z_t}, Q_{z_t}) &
    y_t & \sim \mathcal{N}(x_t, \eps I)
\end{align}
A couple of notes:
\begin{itemize}
    \item Until we migrate to a Bayesian formulation, we do not have parameter priors like those specified in keypoint-moseq or in Section \ref{full-model-derivation}. The EM updates will be simpler to work with, so we're starting in this prior-less regime. We can then derive updates incorporating priors as in Scott Linderman's Stats 305C lecture notes (\href{https://github.com/slinderman/stats305c/blob/spring2023/slides/lecture08-em.pdf}{github}) or Murphy 11.4.2.8 [cite]
    \item Because of the symmetry between $\eps$ and $Q_k$, will fix $\eps$ and treat it as a hyperparameter. Once the morph functions are not identity, we will impose separate priors on uniform keypoint error $R$ and the scale factors of animals, which will break this symmetry. For forward-compatibility, we will work without the assumption $R = \eps I$, however in implementing this model the parameter will be fixed.
    \item In practice, we will reparameterize the model to avoid constrained optimization by taking the mixture weights $\pi$ to be the softmax of an unconstrained cluster weight vector $\bar\pi$.
\end{itemize}
Treating $z_t, x_t$ as latent variables, $\theta = (\pi, m_k, Q_k)$ as a parameter vector, and $\eps$ as a hyperparameter, this model can be fit using expectation maximization, as described in Section \ref{sec:em-general}



\subsection{Expectation maximization}
\label{sec:em-general}

In expectation maximization, jointly optimize the log likelihood $\ell(\theta)$ of a set of model parameters $\theta$ given observations $y$. In particular, we do so while taking the expectation over unknown values of some latent variables --- $z$ and $x$ in our case --- according to an auxiliary distribution $q(x, z | y, \theta^*) \propto P(x, z | y, \theta^*)$ for a current parameter estimate $\theta^*$. The theoretical basis for EM (see Murphy 11.4.7 [cite]) is that taking this expectation produces a lower bound for the likelihood $\ell(\theta)$, namely
\begin{align}
    \ell(\theta) \geq A(\theta, \theta^*) := \EE_{q(x, z | y, \theta^*)} \log P(y, x, z | \theta)
\end{align}
The expectation maximization algorithm splits the problem of iteratively optimizing $\ell$ into two blocks: in the E-step we will calculate $q$ using our current parameter estimates $\theta^*$, so that in the M-step we can calculate $\argmax_\theta A(\theta, \theta^*)$ to arrive at new parameter estimates $\theta^*_{\text{new}}$,
\begin{align}
    \theta^*_{\text{new}} := \argmax_{\theta} \EE_{q(x, z | y, \theta^*)} \log P(y, x, z | \theta) \label{eq:em-argmax}
\end{align}
The key theoretical guarantee is that the new parameter estimates monotonically increase in likelihood, that is $\ell(\theta^*_{\text{new}}) \geq \ell(\theta^*)$.



\textbf{E step:}\ \ The core simplifying assumption from Section \ref{sec:pared-down-gmm} of independence across time allows us to write $q$ as a product distribution of i.i.d. latent variables $x, z$ at each time point, $q_t(x, z) \propto P(x_t = x, z_t = z \mid y_t, \theta^*)$.

Our first move in calculating $q_t$ will be to apply Bayes' rule and drop the denominator, since it does not vary in $\theta$ and therefore cannot affect computation of the $\argmax$ in Equation \ref{eq:em-argmax}:
\begin{align}
    \label{eq:latent-posterior-bayes}
    P(x_t = x, z_t = z \mid y_t, \theta^*) &= \frac{P(y_t \mid x_t = x, z_t = z, \theta^*)P(x_t = x, z_t = z \mid \theta^*)}{P(y_t \mid \theta^*)} \\
    q_t(x, z) &= P(y_t \mid x_t = x, z_t = z, \theta^*)P(x_t = x, z_t = z \mid \theta^*) \label{eq:general-qt}
\end{align}
We will then aim to compute this expression for $q_t$ analytically for each model.

\textbf{M step:}\ \ For simple models, maximization of $A$ can be achieved analytically as well, but in general to improve model iteration speed we will handle the M-step using gradient ascent methods. The goal of our M-steps in this document will therefore be to calculate functional forms of $A$ that are compatible with numerical optimization packages.

Rewriting Eq. \ref{eq:em-argmax} using timepoint independence and expanding the expectation from Eq. \ref{eq:em-argmax}, we arrive at the formula that will be the basis for deriving each model's M-step.
\begin{align}
    \theta^*_{\text{new}} := \argmax_{\theta} \sum_t \sum_{z} \int q_t(x, z)\, \log P(y_t, x, z | \theta) \; dx \label{eq:em-int-argmax}
\end{align}
The main requirement to enable numerical computation and differentiation of $\theta^*_{\text{new}}$ will be to evaluate the integral in $x$. 


\subsection{EM algorithm for the pared-down mixture model}
\label{sec:em-results-pared-down}

In this section we present the results necessary to implement EM for the pared down Gaussian mixture model of Section \ref{sec:pared-down-gmm}. For details, see Section \ref{sec:em-pared-down-derive}. The results of the E-step are implicit in the formulation of the objective function, but we will use the following constants that arise during the E-step:
\begin{align}
    \Sigma_{z, t}^* &= R^* \pn{R^* + Q^*_{z}}\inv Q_{z} \\
    \mu_{z, t}^* &= \Sigma_{z, t}^* \pn{ {R^*}\inv y_t + {Q^*_z}\inv m_{z} } \\
    P(y_t | z, \theta^*) =& \bc{\pn{2\pi}^{-D} \pn{\abs{R^*} \abs{Q_z^*}}\inv \abs{ \Sigma^*_{z, t}}}^{1/2} \times \\
     & \exp\cb{-\frac{1}{2}\pn{y_t^T{R^*}\inv y_t \;+\; {m_z^*}^T{Q_z^*}\inv {m_z^*} \;-\; {\mu_{z, t}^*}^T {\Sigma_{z, t}^*}\inv {\mu_{z, t}^*}}}
\end{align}
which arise as Equations \ref{eq:sigma-star}, \ref{eq:mu-star}, and \ref{eq:y-marg-post} respectively in the derivation. We then proceed in the M-step to numerically maximize the objective function
\begin{align}
    J(\theta) = -\sum_t\sum_z \frac{\pi_z^*\,P(y_t\mid z)}{2} \bigg( 
      & \log\abs{R} + \NN(\mu_{z,t}^* \mid y_t, R) + \Tr\bc{\Sigma_{z, t}^* R\inv} \\
    + & \log\abs{Q_z} + \NN(\mu_{z,t}^* \mid m_z, Q_z) + \Tr\bc{\Sigma_{z, t}^* Q_z\inv} \\
    - &\, 2\bar\pi_k + 2\log\sum \bar\pi \bigg)
\end{align}
whose terms are derived in Equations \ref{eq:obj-term-1}, \ref{eq:obj-term-2}, and \ref{eq:obj-term-3}.






\section{Derivation of EM for the pared-down mixture model}
\label{sec:em-pared-down-derive}

In this section we expand upon parameter inference procedure outlined in Section \ref{sec:em-general} for the pared down Gaussian mixture model of \ref{sec:em-results-pared-down}.

\textbf{E step:}\ \ In the ``expectation'' step, we begin from Equation \ref{eq:general-qt} to derive the auxiliary distribution $q_t(x, z)$. The first action will be to remove unnecessary conditional terms and apply the assumed distributions of the model:
\begin{align}
    \label{eq:latent-posterior-bayes-pared-down}
    q_t(x, z) &= P(y_t \mid x, z, \theta^*)P(x, z \mid \theta^*) \\
    &= P(y_t \mid x, R^*) P(x \mid m^*_{z}, Q^*_{z}) P(z \mid \pi^*) \\
    &= \NN(y_t \mid x, R^*)\, \NN(x \mid m^*_{z}, Q^*_{z})\, \pi_{z}^*
\end{align}
We now move to write the functional form we will use for our auxiliary distribution $q$, which should be an unnormalized distribution equal to the numerator of equation \ref{eq:latent-posterior-bayes}. In general, the product of normal PDFs is proportional to another normal PDF (we will show why the proportionality constant is $P(y_t | z_t, \theta^*)$ momentarily),
\begin{align}
    q_t(x, z) &= \pi_{z}^*\, P(y_t | z, \theta^*)\, \NN(x \mid \mu^*_{z, t}, \Sigma^*_{z, t}) \\
    \Sigma_{z, t}^* &= R^* \pn{R^* + Q^*_{z}}\inv Q_{z} \label{eq:sigma-star} \\
    \mu_{z, t}^* &= \Sigma_{z, t}^* \pn{ {R^*}\inv y_t + {Q^*_z}\inv m_{z} } \label{eq:mu-star}
\end{align}
To understand where our proportionality constant $P(y_t | z, \theta^*)$ arises from, we can marginalize over $x$ using two different forms of the latent posterior $P(x_t, z_t | y_t, \theta^*)$:
\begin{align}
    \int_x P(x, z_t \mid y_t, \theta^*)
    &= P(z_t \mid y_t, \theta^*)
    = \frac{P(y_t | z_t, \theta^*) P(z_t \mid \theta^*)}{P(y_t | \theta^*)}
    =  \frac{P(y_t | z_t, \theta^*) \pi_{z}^*}{P(y_t | \theta^*)}\\
    %
    \int_x P(x, z_t \mid y_t, \theta^*)
    &= \int_x \frac{q_t(x, z_t)}{P(y_t | \theta^*)}
    = \int_x \frac{K \pi_{z}^*\, \NN(x \mid \mu^*_{z, t}, \Sigma^*_{z, t})}{P(y_t | \theta^*)}
    =  \frac{K \pi_{z}^*}{P(y_t | \theta^*)}
\end{align}
In computations, it will be useful to have a more explicit form of this proportionality constant,
\begin{align}
    \label{eq:y-marg-post}
    P(y_t | z, \theta^*) =& \bc{\pn{2\pi}^{-D} \pn{\abs{R^*} \abs{Q_z^*}}\inv \abs{ \Sigma^*_{z, t}}}^{1/2} \times \\
     & \exp\cb{-\frac{1}{2}\pn{y_t^T{R^*}\inv y_t \;+\; {m_z^*}^T{Q_z^*}\inv {m_z^*} \;-\; {\mu_{z, t}^*}^T {\Sigma_{z, t}^*}\inv {\mu_{z, t}^*}}}
\end{align}
which we derive in an abstract setting to simplify notation:
\begin{proposition}
    The product normal PDFs evaluated at a point, $N_a = \NN(x\mid a, A)$ and $N_b = \NN(x\mid b, B)$, is proportional to $N_c = \NN(x\mid c, C)$ with and $C = A\pn{A + B}\inv B$ and $c = CA\inv a + CB\inv b$. Moreover, if $Z_\Sigma = \pn{2\pi}^{-D/2}\abs{\Sigma}^{-1/2}$ is the usual Gaussian normalization factor for covariance matrix $\Sigma$, then equality is achieved using the following proportionality constant:
    \begin{align}
        N_a N_b = \frac{Z_aZ_b}{Z_c} \exp\cb{-\frac{1}{2}\pn{a^TA\inv a \,+\, b^TB\inv b \,-\, c^T C\inv c}} N_c.
    \end{align}
\end{proposition}
\begin{proof}
The proportionality result is standard, so we leave that proof to the reader and use the result to derive our proportionality constant. Let $E_i$ be the exponent in the normal PDF $N_i$, namely $E_a = (x - a)^TA\inv (x - a)$. Then the exponents of $N_aN_b$, $E_a + E_b$, and the exponent of $N_c$, $E_c$ only differ in those terms which are constant in $x$, i.e., 
\begin{align}
    \pn{E_a + E_b} - E_c = a^TA\inv a + b^TB\inv b + c^T C\inv c
\end{align}
Finally using the Gaussian PDF normalization constant $Z_\Sigma = \pn{2\pi}^{-D/2}\abs{\Sigma}^{-1/2}$, we can write the constant $K$ such that $N_aN_b = K N_c$ as $\frac{Z_aZ_b}{Z_c} \exp\cb{-\frac{1}{2}\pn{E_a + E_b - E_c}}$.
\end{proof}

\textbf{M step:}\ \ In our ``maximization'' step, we make $A(\theta, \theta^*)$ numerically computable to enable gradient ascent optimization. In particular, we must evaluate the integral w.r.t. $x$ that appears in Equation \ref{eq:em-int-argmax}. These derivations closely follow the standard ones for expectation maximization of a Gaussian mixture model. Expanding the joint probability in equation \ref{eq:em-int-argmax}, we arrive at three terms that to integrate against $q_t$:
\begin{align}
    \argmax_{\theta} \sum_{t}\sum_{z}\int_{x} q_t(x, z) \bc{ \log P(y_t \mid x, R) + \log P(x \mid m_{z}, Q_{z}) + \log P(z \mid \pi)}
\end{align}
Note that we may optimize many of the parameters separately, since each $\log P$ is constant in all but a few of them. In particular, we may find an analytical optimum for $\pi$, which requires constrained optimization, and then continue with our numerical optimization procedure for other variables, meaning that we do not need to evaluate the integral on the third term.

\textit{Integration against $\log P(y_t \mid x, R)$.}\ \ We aim to evaluate $\int_x q_t(x, z) \log P(y_t \mid x, R)$ for a given $z,t$. By applying the model assumption that $y_t$ is normally distributed with parameters $x, R$, the log probability may be expanded as:
\begin{align}
    \pi_{z}^*\, P(y_t | z, \theta^*) \int_x \NN(x \mid \mu^*_{z, t}, \Sigma^*_{z, t}) \bigg[
        &-\frac{D}{2}\log(2\pi) \label{eq:const-theta-drop-1}\\
        &- \frac{1}{2}\log \abs{R} \label{eq:marginalize-drop-1} \\
        &- \frac{1}{2}(y_t - x)^T R\inv (y_t - x) \bigg] \label{eq:gauss-quad-expect-1}
\end{align}
The first term (Eq. \ref{eq:const-theta-drop-1}) is constant in $\theta$ and therefore may be dropped. The second (Eq. \ref{eq:marginalize-drop-1}) is constant in $x$, so integration against a normal PDF in $x$ is identity. For the third term (Eq. \ref{eq:gauss-quad-expect-1}), we apply a general formula for integration of a quadratic form against a normal PDF:

\begin{proposition}
The expectation of a quadratic form in a normal variable is the sum of a normal PDF and a trace:
\begin{align}
    \EE_{x\sim\NN(a, B)} \bc{(x - c)^T D\inv (x - c)} = \NN(a \mid c, D) + \Tr\bc{BD\inv}
\end{align}
\label{prop:norm-quad-expect}
\end{proposition}

The proof is left to the reader, but is achieved by wrapping the whole expectation in a trace and cycling its arguments to arrive at an outer product $(x - c)(x - c)^T$. Combining the observations above, we arrive at 
\begin{align}
    \int_x q_t(x, z) \log P(y_t \mid x, R) = -\frac{1}{2}\pi_{z}^*\, P(y_t | z, \theta^*) \pn{ \log \abs{R} + \NN(\mu_{z,t}^* \mid y_t, R) + \Tr\bc{\Sigma_{z,t}^* R\inv} }
    \label{eq:obj-term-1}
\end{align}

\textit{Integration against $\log P(x \mid m_{z}, Q_{z})$.}\ \ Next we calculate $\int_x q_t(x, z) \log P(x \mid m_{z}, Q_{z})$ for a given $z,t$. The procedure is the same as for $P(y_t \mid x, R)$ but for a normal with parameters $m_{z}, Q_{z}$, and results in:
\begin{align}
    \int_x q_t(x, z) P(x \mid m_{z}, Q_{z}) = -\frac{1}{2}\pi_{z}^*\, P(y_t | z, \theta^*) \pn{ \log \abs{Q_z} + \NN(\mu_{z,t}^* \mid m_z, Q_z) + \Tr\bc{\Sigma_{z,t}^* Q_z\inv} }
    \label{eq:obj-term-2}
\end{align}

\textit{Integration against $\log P(z \mid \pi$.}\ \ Finally, we calculate $\int_x q_t(x, z) \log P(z \mid \pi)$ for a given $z, t$. Because the log probability does not depend on $x$, the integral marginalizes the normal PDF in $q_t(x, x)$, and we arrive at $\pi_{z}^*\, P(y_t | z, \theta^*) \, \log \pi_k$. Substituting then the logits vector $\bar\pi$ results in the form amenable to unconstrained optimization:
\begin{align}
    \int_x q_t(x, z) \log P(z \mid \pi) = \pi_{z}^*\, P(y_t | z, \theta^*) \pn{ \bar\pi_k - \log\sum_i \bar\pi_i }
    \label{eq:obj-term-3}
\end{align}

\label{full-model-derivation}
\section{Optimizing the mixture of linear Gaussians}

We assume the following generative model:
\begin{align}
    z_t^i & \sim \text{Cat}(\pi^i) && 
    \pi^i \sim \text{Dir}(\alpha) \\
    x_t^i & \sim \mathcal{N}(m_{z_t^i}, Q_{z_t^i}) &&
    m_n, Q_n \sim \text{Normal-Inverse-Wishart} \\
    y_t^i & \sim \mathcal{N}(F^i x_t^i + d^i, I_{KD}) && 
    d^i, F^i \sim \text{Matrix-Normal}   
\end{align}
%
where $i$ indexes animals, $t$ indexes frames, $x_t^i, y_t^i \in \mathbb{R}^{KD}$, $z_t^i \in \{1,...,N\}$, and $\sigma^2 \in \mathbb{R}^+$ is fixed. The parameters can be optimized using variation mean field EM, in which the posterior over latent variables is approximated by the product of factors $q(z_t^i), q(x_t^i)$. During the M-step, we find the parameters $\theta = (m, Q, d, F, \pi)$  that maximize $\mathbb{E}_{q(x,z)} \log P(y,x,z | \theta)$, where $q(x,z) = \prod_{t,i} q(z_t^i) q(x_t^i)$. During the E-step, we iteratively update the factors $q(z_t^i), q(x_t^i)$ using coordinate ascent, with one or more passes through the data per iteration. The updates are given by:
\begin{align}
\log q^*(z_t^i) & = \mathbb{E}_{q(x_t^i)} \log P(y_t^i, x_t^i, z_t^i | \theta) + \text{const.} \\
& = \int_{x_t^i} \mathcal{N}(x_t^i \mid \mu, \Sigma) \log \mathcal{N}(x_t^i \mid m_{z_t^i}, Q_{z_t^i}) + \log \pi^i_{z_t^i} + \text{const.} \\
& = \mathcal{N}(\mu \mid m_{z_t}^i, Q_{z_t}^i) - \tr [Q_{z_t^i}^{-1}\Sigma] + \log \pi^i_{z_t^i} + \text{const.}
\end{align}



\end{document}