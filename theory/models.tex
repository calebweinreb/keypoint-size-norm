\documentclass{article}         %% What type of document you're writing.
\usepackage[fleqn]{amsmath}
\usepackage{amsfonts,amssymb}   %% AMS mathematics macros
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amssymb}
\usepackage{parskip}
\usepackage{mathtools}
\usepackage{bm}
\usepackage[margin=1.3in]{geometry}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{tikz}
%\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}
\usetikzlibrary{backgrounds}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newcommand\descitem[1]{\item{\bfseries #1}}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\argmax}{\arg\max}
\DeclareMathOperator{\pad}{pad}
\graphicspath{ {./figs/} }
\newcommand{\inv}{^{-1}}
\newcommand{\pd}{\partial}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\mat}[1]{\begin{matrix} #1 \end{matrix}}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\left|\left| #1 \right|\right|}
\newcommand{\cb}[1]{\left\{ #1 \right\}}
\newcommand{\pn}[1]{\left( #1 \right)}
\newcommand{\bc}[1]{\left[ #1 \right]}
\renewcommand{\cal}[1]{\mathcal{#1}}
\newcommand{\eps}{\varepsilon}
\DeclareMathOperator{\Tr}{Tr}
\usepackage{titling}

\title{Size norm model ideas}
\author{Caleb Weinreb and Kai Fox}
\date{May 2023}

\begin{document}

\maketitle

\section{Modeling framework}
\label{sec:model-fwk}

Suppose we have pose data $\{y_t\}_{t=1}^T$ and $\{y'_t\}_{t=1}^{T'}$ for a pair of animals, where $y_t, y'_t \in \mathbb{R}^{KD}$, represent the positions of $K$ keypoints in $D$ dimensions. For now, we will assume that the keypoints are in egocentric coordinates, meaning each animal is always centered and pointing in the same direction. Our goal is to define a canonical (low-dimensional) pose space that does not reflect body shape, and learn how to the data from each animal map onto it. Put in terms of generative modeling, we wish to explain each animal $n$ observed pose at time $t$, $y_n^t$, as the (noisy) realization of some latent pose state $x_n^t \in \mathbb{R}^M$, where the space of latent states is shared between animals.
%
\begin{align}
    y_1^t & = f_1(x_1^t) + \xi \ \text{where} \ 
    \xi \sim \mathcal{N}(0, R) \ \text{and} \ 
    x_1^t \sim P_1 \\
    y_2^t & = f_2(x_2^t) + \xi \ \text{where} \ 
    \xi \sim \mathcal{N}(0, R) \ \text{and} \ 
    x_2^t \sim P_2
\end{align}
%
where $f_1, f_2$ are respective \textit{morph} functions mapping from the latent space to each animal's pose space, and $P_1, P_2$ are distributions over latent pose states. Ideally, we want $f_1$ and $f_2$ to capture morphological differences between the two animals, and $P_1, P_2$ to capture differences in the frequency of behaviors.

This formulation explicitly puts $f$ and $P$ in competition to explain variation in observed keypoint data as differences subject-wise body morphology or differences in occupancy of pose space. For example --- an animal with higher density in observation space where keypoints on the back are far apart could be due to a larger body or due to an upregulation of exploratory behavior such as rearing.

We want to make sure that $f_1$ and $f_2$ are as similar as possible, i.e. to avoid mapping similar regions of latent pose space onto completely different bodily realizations for different animals or ``collapsing'' truly different observed postures onto the same latent poses. However, we also wish to keep $P_1$, $P_2$ as similar as possible to avoid trivial explanations that fail to unify variation across animals in the latent space. These desires will be encoded both in structural constraints on $f$ and $P$ such as the number of degrees of freedom in their parameterizations, and in hyperparameters on their parameter priors. Later work will specify a battery of controls to interrogate the correct amount of flexibility to afford $f$ and $P$ respectively for a given dataset.

We take the approach of formulating general results with minimal specifications on $f$ and $P$, and developing a set of responsibilities that a particular \textit{pose space model} $P$ or \textit{morph model} $f$ must perform. This modular framework allows us to examine the ways that implicit assumptions in the structure of each model cause them to trade off explanation of the observed keypoint distributions. We will however work within a limited set of constraints on the kinds of models we consider.
\begin{itemize}
\item \textit{Affine morphs}. We assume that morphs may be written $f(x) = C x + d$ for a matrix $C\in \RR^{KD\times M}$ and a vector $d\in \RR^{KD}$. Note that linearity in this sense does not entail simple scaling and shearing in $D$-dimensional keypoint space, but should rather be thought of similarly to the linearity of PCA in that certain directions or \textit{modes} in latent pose space will correspond to certain postures in keypoint space. The specification of a different morph for each animal allows us to define the way those modes are realized on each body.

\item \textit{Time independence}. A core simplifying assumption of all of our pose space models $F_n$ will be that $x^t$ for a given animal are i.i.d. over timesteps. In this way our model is a deepening of keypoint-moseq's affine transform from pose space to keypoint space $y \sim \NN(Cx+d, R)$, where we vary the transform per-mouse and investigate other transforms, while ignoring all dynamical character of behavior. 

\item \textit{Phoneme variation.} We will consider pose space models that are common across animals, conditional upon an additional discrete latent variable $z$ specifying pose ``phonemes'' (\textit{po-nemes}, \textit{postu-nemes}?), much as the discrete variable in keypoint-moseq describes dynamical syllables. We allow the distribution of the discrete latent $z$ to vary per-animal, such that the flexibility of the pose space distribution $P$ is in defining the frequency of occupying a set of phonemes that are shared (up to body morphology) across the population.
\end{itemize}
With these structural assumptions, we may more formalize our generative modeling framework in more detail. 

\textbf{Pose space:}\ \ A pose space model in our framework is a joint distribution over $x$ and $z$ for a single timepoint (extended i.i.d. to sequences). We parameterize such a model by $\gamma$ defining per-animal distributions $G_n$ of the discrete phoneme latent $z$, and parameters $\psi$ defining the distribution $F$ of latent pose states $x$, conditional on $z$. In this way a pose space model is given by a tuple of PDFs $(F, G_n)$ that together specify
\begin{align}
    P_n(x, z) &= F(x \mid z, \psi)\,G_n(z \mid \gamma)
    % F(\cdot \mid z, \psi)&:\ \RR^M \rightarrow \RR,\ \ G(\cdot \mid \gamma): \cal{Z} \rightarrow \RR
\end{align}

\textbf{Morph:}\ \ A morph model in our framework is a per-animal affine map from latent pose space to noisy observations in keypoint space. We parameterize this map by a tuple $\phi$ which defines a matrix $C_n$ and a vector $d_n$ for subject $n$. A pose space model is therefore given by a tuple of functions $(C_n, d_n)$ mapping $\phi$ to $\RR^{KD\times M}$ and $\RR^{KD}$ respectively, that together specify
\begin{align}
    f_n(x_n^t) &= C_n(\phi)\, x_n^t + d_n(\phi)
\end{align}

The total parameter set for the combined model, including the observation covariance $R$, is therefore $\theta = (\gamma, \psi, \theta, R)$. The full model may therefore be written
\begin{align}
    z^t_n &\sim G_n(\,\cdot \mid \gamma) &
    x^t_n &\sim F(\,\cdot \mid z^t_n, \psi) &
    y^t_n &\sim \NN(C_n(\phi)\, x_n^t + d_n(\phi), R)
    \label{eq:gen-model}
\end{align}
As we will see below, each morph model must simply compute the affine transformation specified by $C_n, d_n$, while most of the heavily lifting is left to the pose space models. In addition, we consider a log-prior defined separately for parameters of the pose and morph models $\log P(\theta) = L_{\text{pose}}(\gamma, \psi, R) + L_{\text{morph}}(\phi)$.


\subsection{Expectation maximization}
\label{sec:em-general}

We fit our models using expectation maximization (EM). That is, we learn model parameters $\theta$ that maximize the log likelihood $\ell(\theta) = \log P(y, x, z\mid \theta)$ of all observed keypoints $y$ and latent pose states $(x, z)$ under particular set of model parameters $\theta$. In particular, we do so by taking the expectation of the log likelihood over the unknown values the latent pose states $(x, z)$ according to an auxiliary distribution $q(x, z | y, \theta^*) \propto P(x, z | y, \theta^*)$ defined by a current parameter estimate $\theta^*$. The theoretical basis for EM (see Murphy 11.4.7 [cite]) is that taking this expectation produces a lower bound for the likelihood $\ell(\theta)$. Namely, given a prior $P(\theta)$,
\begin{align}
    \ell(\theta) \geq A(\theta, \theta^*) := \EE_{q(x, z | y, \theta^*)} \log P(y, x, z | \theta) + \log P(\theta)
    \label{eq:A-defn}
\end{align}x
The expectation maximization algorithm splits the problem of iteratively optimizing $\ell$ into two blocks: in the ``expectation'' step (E-step) we will calculate $q$ using our current parameter estimates $\theta^*$, so that in the ``maximization'' step (M-step) we can calculate $\argmax_\theta A(\theta, \theta^*)$ to arrive at new parameter estimates $\theta^*_{\text{new}}$:
\begin{align}
    \theta^*_{\text{new}} := \argmax_{\theta} \EE_{q(x, z | y, \theta^*)} \log P(y, x, z | \theta) + \log P(\theta)\label{eq:em-argmax}
\end{align}
The key theoretical guarantee is that the new parameter estimates monotonically increase in likelihood, that is $\ell(\theta^*_{\text{new}}) \geq \ell(\theta^*)$.

The core simplifying assumption from Section \ref{sec:model-fwk} of independence across time allows us to write $q$ as a product distribution of i.i.d. latent variables $x, z$ at each time point. We write $q_n^t(x, z) = P(x_n^t = x, z_n^t = z \mid y_n^t, \theta^*)$. Rewriting Eq. \ref{eq:em-argmax} using this timepoint independence and expanding the expectation as an integral, we arrive at the formula that will be the basis for deriving each model's parameter updates:
\begin{align}
    \theta^*_{\text{new}} := \argmax_{\theta} \sum_t \sum_{z} \int q_n^t(x, z)\, \log P(y_t, x, z | \theta) \; dx + \log P(\theta)\label{eq:em-int-argmax}
\end{align}

For simple models, maximization of $A$ can be achieved analytically, but in general to improve model iteration speed we will handle the M-step using gradient ascent methods. The goal of our M-steps in this document will therefore be to calculate functional forms of $A$ that are compatible with numerical optimization packages. In particular, we must handle integrals of the log probability $\log P(y_t, x, z\mid \theta)$ against $q_n^t(x, z)$ analytically. This means also that our E-step will not take the standard form of computing probabilities $q_n^t$ for each value of the latent state $(x, z)$, but instead will involve precomputing values for the integral that are independent of $\theta$.


\subsection{Responsibilities of morph and pose space models in EM}

The majority of the calculations in both the E- and M-steps can be performed in terms of the affine map $(C_n, d_n)$ and therefore may be posed as responsibilities of the pose space model, given the affine map produced by the morph model as input. The only responsibilities of the morph model are to calculate these matrices and vectors, evaluate priors and initializations for the parameters, and to provide convenience functions such as finding maximum likelihood estimates of latents given observations and parameters.

The requirements of the pose space model on the other hand are to calculate priors, initializations, and convenience functions as the morph model does, but also to evaluate the objective function $A$ of the maximization routine in Eq. \ref{eq:em-int-argmax}. We now expand upon the framework for analytically integrating our problematic integral. In doing so we enumerate both the necessary arithmetic results to derive for the M-step of each pose space model, and the computations to be performed in their E-steps.

The theoretical guarantees of EM are achieved by taking the latent distribution to be the log likelihood of the latents given the observations and current parameter estimates $\theta^*$, which is usually calculated using Bayes' rule
\begin{align}
    q_n^t(x, z) = P(x, z \mid y_n^t, \theta^*) &= \frac{P(y_n^t, x \mid z, \theta^*) P(z\mid \theta^*)}{P(y_n^t \mid \theta^*)}
    \label{eq:q-general}
\end{align}
For integration in $x$, this fraction breaks down as a normalizer times $P(y_n^t, x \mid z, \theta^*)$. As shown below, that probability the product the normal PDF around $f(x)$ giving the probability of the noisy observation $y_n^t$ and the probability of the pose state $x$ given by $F$. It will be the central task of the M-step to rewrite that product in turn as a constant in $x$ times an auxiliary probability distribution $s$ to be integrated against the data likelihood $\log P(y_n^t, x, z \mid \theta)$:
\begin{align}
    P(y_n^t, x \mid z, \theta^*) &= P(y_n^t \mid x, \theta^*) P(x\mid z, \theta^*) \\
    &= \NN(y_n^t\mid C_n(\phi^*)\, x + d_n(\phi^*), R^*) F(x\mid z, \psi^*) \label{eq:norm-F-prod} \\
    &:=  K_{y_n^t, z, \theta^*}\cdot s(x;\, y_n^t, z, \theta^*)
    \label{eq:s-defn}
\end{align}
We expand upon the requirement that $s(x)$ be a PDF in which expectations of the log probability terms arising from the data likelihood are known below in %\ref{sec:}
Note that for brevity we write $s, K$ as functions of $\theta$, however by Eq. \ref{eq:norm-F-prod} they are functions only of $(\psi^*, R^*, C_n(\phi^*), d_n(\phi^*))$ and therefore may be evaluated by the morph model operating only on the affine transform output by an arbitrary pose space model.

We find that these constants $K$ and expectations in $s$, together with the finite array of probabilities for the discrete latents $G_n(z \mid \gamma)$, are sufficient to express the objective function $A(\theta; \theta^*)$. The remaining term in the numerator of $q_n^t$ in Eq. \ref{eq:q-general}, $P(z\mid \theta^*)$, is simply $G(z\mid \gamma^*)$. Additionally, marginalizing Eq. \ref{eq:s-defn} over $x$ shows that the constants $K$ are in fact the probabilities $P(y_n^t \mid z, \theta^*)$, so the denominator $P(y_n^t \mid \theta^*)$ of $q_n^t$ in Eq. \ref{eq:q-general} may be expanded by the law of total probability as $\sum_z K_{y_n^t, z, \theta^*} G(z\mid \gamma^*)$. The objective function as defined in Eq. \ref{eq:A-defn} may therefore be written
\begin{equation}
    A(\theta; \theta^*) = \sum_{t, n}\sum_z  \frac{K_{y^t_n, z, \theta^*} G(z\mid \gamma^*)}{\sum_{z'} K_{y^t_n, z', \theta^*}  G(z'\mid\gamma^*)}\ \EE_{s(x;\, y^t_n, z, \theta^*)}\bc{\log P(y^t_n, x, z \mid \theta)} + \log P(\theta) \label{eq:A-tractable}
\end{equation}

Using Eq. \ref{eq:A-tractable}, we may state explicitly the arithmetic work to be done in deriving a pose space model and the computations that a pose space model must perform during an EM iteration in our modeling framework:

\textbf{Derivation:}\ \ A pose space model must define $(F, G)$, and rewrite the product of $F$ with a Gaussian probability as a constant $K$ times a PDF $s(x)$, as in Eq. \ref{eq:s-defn}. Closed form integrals of $s$ against the two terms outlined in Sec. \ref{sec:s} must be derived.

\textbf{E step:}\ \  A pose space model must, during the E-step, calculate the constants $K_{y_n^t,z,\theta^*}$ as a function of the estimated parameters $\theta^*$ resulting in an $\RR^{|y|\times |z|}$ matrix, where $|y|$ is the number of observed samples and $|z|$ is the cardinality of the discrete latent ``phoneme'' space. The function $s$ will be defined parametrically, and during the E-step a pose space model must also calculate the $\RR^{|y|\times |z|}$ matrix of parameters as a function of $\theta^*$.

\textbf{M step:}\ \ A pose space model will use the derived closed form expectations as functions of $\theta$ and $s$ (parameterized as the results of the E-step) alongside the constants $K$ calculated in the E-step to compute the objective function Eq. \ref{eq:A-tractable} during the M-step. General model-agnostic methods will perform gradient-based maximization based on this output.

\subsection{Requirements for the auxiliary distribution of a pose model}
\label{sec:s}

It is crucial for each pose space model to formulate an auxiliary PDF $s$ for which the expectations in the rewritten objective function (Eq. \ref{eq:A-tractable}) have a closed form expression. To elaborate on this constraint, we expand the log probability of the data using the structure of our generative model (Eq. \ref{eq:gen-model}): 
\begin{align}
    \log P(y^t_n, x, z \mid \theta) &= \log P(y^t_n \mid x, \theta) + \log P(x\mid z, \theta) + \log P(z\mid \theta) \\
    &= \log \NN(y^t_n \mid C_n(\phi)\, x + d_n(\phi), R) + \log F(x\mid z,\psi) + \log G(z \mid \gamma)
    \label{eq:logprob-terms}
\end{align}
Of these, only the first two are functions of $x$ and thereby impose a requirement on $s(x)$ of having a known closed form expectation.

It will in general be necessary when finding the closed form expectations to recognize that the first term may be written as a normal PDF in $x$ by transforming the normal distribution of $y$ around $f(x)$ to a scaled normal distribution of $x$ around $f\inv(y)$. Let $\norm{a - b}_A$ is the Mahalanobis distance of $a, b$ according to covariance $A$. Then, dropping the dependence of $C_n, d_n$ on $\phi$ for readability,
\begin{align}
    \log \NN(y\mid C_n\, x + d_n, R) &= \log\pn{\frac{Z_{R}}{Z_{C_n\inv R {C_n^T}\inv}} \NN\pn{x\mid C_n\inv \pn{y - d_n},\; C_n\inv R\, {C_n^T}\inv}} \\ &= \log Z_{R} - \frac{1}{2}\norm{x - C_n\inv (y - d_n)}^2_{C_n\inv R\, {C_n^T}\inv}
    \label{eq:cn-swap}
\end{align}
The result is an unnormalized Gaussian log probability to be integrated against $s(x)$ and a normalizer term that may be computed independent of the morph model. For completeness, we now explicate a proof general inversion of an affine-transformed Gaussian PDF:
\begin{proof} For vectors $x\in \RR^M$ and $y\in \RR^N$ and matrices $A\in \RR^{N\times M}, B\in \RR^{N\times N}$, we may expand a normal PDF as a normalizer times a squared Mahalanobis distance:
    \begin{align}
        \NN(y\mid Ax, B) &= Z_{B} \exp\cb{-\frac{1}{2}\norm{y - Ax}^2_{B}} \\
        &= Z_{B} \exp\cb{-\frac{1}{2}(y - Ax)^TB\inv(y - Ax)} \\
        &= Z_{B} \exp\cb{-\frac{1}{2}(A\inv y - x)^TA^TB\inv A(A\inv y - x)} \\
        &= Z_{B} \exp\cb{-\frac{1}{2}\norm{A\inv y - x}^2_{A\inv B{A\inv}^T}} \\
        &= \frac{Z_{B}}{Z_{A\inv B{A\inv}^T}} \NN\pn{x\mid A\inv y, \; A\inv B{A\inv}^T}
    \end{align}
Additionally, for $x,y,z$ in $\RR^N$ and $B\in\RR^{N\times N}$, $\NN(x - z \mid y, B) = \NN(x \mid y+z, B)$. Applying these two identities yields Eq. \ref{eq:cn-swap}.
\end{proof}

To summarize, during the derivation of a pose space model, expectations in the random variable $x\sim s$ of the following terms must be derived as closed-form expressions. Note that while $s$ is defined in terms of estimated parameters $\theta^*$, the following terms are functions of test parameters $\theta$ during gradient based optimization of $A$.
\begin{itemize}
    \item The Gaussian log-PDF centered at morphed latent poses $f(x) = C_n x + d_n$ with observation covariance $R$ evaluated at observations $y_n^t$. This PDF breaks down as in Eq. \ref{eq:cn-swap} into a quadratic form and a Gaussian normalizer $\log Z_R$ that is constant in $x$.
    \item The log-PDF of the pose space distribution $F(x\mid z, \psi)$ at a latent pose $x$ under given discrete pose state $z$ and test parameters $\psi$.
\end{itemize}


\section{Morph models}

\subsection{Scalar morph}

Before introducing spatially nonlinear morphs, we begin by working with a simple uniform scale around an affine offset. Letting the morph model be parameterized by scalars $\alpha_n$ and offsets $\mu_n\in \RR^N$, we take $C_n(\phi) = e^{\alpha_n} I_{KD}$, and $d_n(\phi) = \mu_n$. Using this scalar morph the latent pose space dimension cannot compress keypoint space, so $M = KD$.

\textbf{Priors}\quad As will be standard for most morph models, we define a prior to remove ambiguity both in the offsets $\mu_n$ and the scale parameters $\alpha_n$. In particular, if $\bar{\mu}(\phi)$, $\bar{\alpha}(\phi)$ are the average offset $\EE_n\bc{\mu_n(\phi)}$ and log-scale, respectively, then the prior on $\phi$ for the scalar morph is
\begin{align}
    L_{\text{morph}}(\phi) &= \log \NN\pn{\bar{\mu}(\phi) \mid 0, I_{KD}} + \log \NN\pn{\bar{\alpha}(\phi) \mid 0, I_1} \\
    &= -\frac{1}{2}\norm{\bar{\mu}(\phi)}_2^2 - \frac{1}{2}\bar{\alpha}(\phi)^2
    \label{eq:d-alpha-prior}
\end{align}

\textbf{Initialization}\quad We initialize offsets to the subject-wise mean in keypoint space $\mu_n = \EE_t[y^t_n]$ and log-scale factors to the MLE of standard deviation for a spherical Gaussian centered at $\mu_n$ given the subject's keypoint data, $\alpha_n = \frac{1}{2}\log\EE_t[\norm{y^t_n - \mu_n}_2^2]$

\subsection{Affine modal alignment}

The first (dare I say ...only?) spatially non-rigid morph we will consider is an affine linear transform which aligns a limited number of dimensions around a centroid. We first give the intuition for this model, then define it and specify priors and an initialization routine.

As in the scalar morph model, we make the assumption that all latent poses for a subject are related to their keypoints by the same offset and scaling, i.e. that there is a certain homologous centroid pose that varies per-animal and that each animal is characterized by a scale of variations from that centroid. In addition, we consider that there might be variations from the centroid that differ by subject but should also be considered homologous --- for example large mice might turn more sharply to follow the same path, or that the limbs of older mice might take flatter trajectories when walking if their bodies are held lower to the ground. We therefore allow variation of a certain number, $L$, of posture \textit{modes} to be mapped differently from latent pose space to each animal's body. We may represent a pose as a magnitude of variation from the centroid along each of these dimensions (as is done in PCA) along with an additional $(KD-L)$-dimensional component describing the remaining variation, which we leave unaltered when mapping from pose space to keypoint space.

Let $L$ be an integer d.o.f. hyperparameter giving the number of pose modes to be aligned. In this case, we define the morph model in terms of the following parameters
\begin{itemize}
    \item $\bar\alpha\in\RR$ - log of the subject-wise uniform scale factor $\alpha = e^{\bar{\alpha}}$ to be applied to all dimensions of pose space
    \item $U\in \RR^{KD\times L}$ - matrix whose columns give the population-wide modes in pose space that may be morphed differently per-animal. \textit{In the special case where the columns of $U$ are the first $L$ principal components and are considered a hyperparameter instead of being learned, we call this morph a ``PC modal alignment''}.
    \item $\hat{U}_n\in\RR^{KD \times L}$ - matrix whose columns, $\hat{u_{n, l}}$, give subject-wise updates to the morph dimensions.
    \item $\mu_n\in \RR^{KD}$ - center of the affine transform, so that $d_n(\phi) = \mu_n$. 
\end{itemize}

Let $\bar{U}$ be the projection on to the orthogonal complement of $U$, which will be used as a pass-through for those dimensions of pose space that should be effected only by uniform scaling. Using these parameters, the modal alignment morph is the affine map given by the matrix $C_n(\phi)$ and the vector $d_n(\phi)$:
\begin{align}
    C_n(\phi) &= \alpha_n \bc{ \bar{U} + \pn{U + \hat{U}_n}U^+ } &\text{\ with\ } \phi &= (U, \bar\alpha_n, \hat{U}_n, \mu_n,\ n\in1...N)
    \\
    d(\phi_n) &= \mu_n & \alpha_n &= \exp\pn{\bar\alpha_n}
    \\
    && \hat{u}_{n, l} &= (\hat{U}_{n})_{:,l} \sim \NN(0, \upsilon^2 I_{KD}) 
\end{align}


To understand the action of the morph, consider the $L=2$ case in which the columns of $U$, which we call $u_1$ and $u_2$, are orthonormal. Let $\hat{u}_1, \hat{u}_2$ be the updates to the morph dimensions for subject $n$. Then the action of $C(\phi_n)$ on $\text{span}(u_1, u_2)$ - the subspace spanned by the morph dimensions - is a perturbation of the symmetric matrix with SVD $U (\alpha I_{KD}) U^T$, namely
\begin{align}
    C(\phi_n) : x\in \text{span(U)} \mapsto \bc{\mat{ | & | \\ u_1 + \hat{u}_1 & u_2 + \hat{u}_2 \\ | & |  }}
    \bc{\mat{ \alpha & \\ & \alpha }}
    \bc{\mat{ - & u_1^T & - \\ - & u_2^T & -}}x
\end{align}
The action of $C(\phi_n)$ on the orthogonal complement of $U$ should then be the identity. Removing the requirement of orthogonal morph dimensions, and considering vectors outside the span of those morph dimensions, we have the general expression:
\begin{align}
    C(\phi_n) = \alpha \bc{ \bar{U} + \pn{U + \hat{U}_n}\text{Diag}\pn{s_n}U^+ }
\end{align}
with $U^+$ being the Moore-Penrose pseudoinverse of U.

\textbf{Priors}\quad We inherit the normal priors on the average offset and log-scale to remove ambiguity in $\mu_n$ and $\alpha_n$ as in Eq. \ref{eq:d-alpha-prior}. In addition, we include a Gaussian prior on the mode adjustments, the columns of the matrix $\hat{U}_n$ which we call $u_{n,l}$,  after scaling by the norm of the corresponding mode $u_l$, the $l$-th column of the matrix $U$, according to a scale hyperparameter $\upsilon$. Finally, we remove symmetry in $\norm{u_l}$ and arrive at the following morph parameter log likelihood function:
\begin{align}
    L_{\text{morph}}(\phi) =& \log \NN\pn{\bar{\mu}(\phi) \mid 0, I_{KD}} + \log \NN\pn{\bar{\alpha}(\phi) \mid 0, I_1} \\
    &+ \sum_{n,l=1}^{N,L}\log \NN(u_{n,l}, 0, \upsilon^2 I_{KD}) + \sum_{l=1}^L \log\NN(\log \norm{u_l}, 0, 1)\\
    &= -\frac{1}{2}\norm{\bar{\mu}(\phi)}_2^2 - \frac{1}{2}\bar{\alpha}(\phi)^2 - \frac{1}{2\upsilon^{2KD}}\sum_{n,l=1}^{N,L}\norm{u_{n, l}}_2^2 \norm{u_l}_2^{-2} - \frac{1}{2}\sum_{l=0}^1 \log^2 \norm{u_l}
\end{align}



\section{Pose space models}


\subsection{Gaussian mixture}

In the Gaussian mixture model, our discrete latent ``phoneme'' state is categorical over $L$ values  (a hyperparameter) which we term components as in a standard GMM. The discrete distribution weights subject to a heirarchical Dirichlet prior with hyperparameters for usage uniformity across components, $\hat\beta$, component usage uniformity across animals $\hat\pi$, and a saturation level for logits $\pi_{\text{max}}$. Our continuous pose state is normal, with mean and covariance conditional on the discrete state.
\begin{align*}
    G\pn{z \mid \gamma} &= \text{Cat}(\pi_n) & \text{with\ }&\gamma = (\bar\beta\in \RR^L, \bar\pi_n \in \RR^L,\ n\in1...N) \\
    &&& \beta = \text{softmax}(\bar\beta),\ \beta \sim \text{Dir}(\hat\beta) \\
    &&& \pi_n = \text{softmax}( \pi_{\text{max}}\text{tanh}( \bar\pi_n / \pi_{\text{max}})),\ \pi_n \sim \text{Dir}(\hat\pi\;\gamma) \\
    F(x \mid z, \psi) &= \NN(m_z, Q_z) & \text{with\ }&\psi = ((m_z, Q_z) \in \RR^{M} \times \RR^{M\times M},\ z\in 1 ... L)
\end{align*}


\subsubsection{Auxiliary PDF and constants}

Our first task is to rewrite the product of $F$ and a normal PDF in $y_n^t$, from Eq. \ref{eq:norm-F-prod}, as a constant $K$ times a PDF $s(x)$. Since $F$ is itself Gaussian in this case, we may apply the same transformation as in \ref{eq:cn-swap} to arrive at a product of two Gaussians in $x$, which is proportional to another normal PDF in $x$. The following proposition specifies the proportionality constant.
\begin{proposition}
    The product normal PDFs evaluated at a point, $\NN(x\mid a, A)$ and $\NN(x\mid b, B)$, is proportional to $ \NN(x\mid c, C)$ with and $C = A\pn{A + B}\inv B$ and $c = CA\inv a + CB\inv b$. Moreover, if $Z_\Sigma = \pn{2\pi}^{-D/2}\abs{\Sigma}^{-1/2}$ is the usual Gaussian normalization factor for covariance matrix $\Sigma$, then equality is achieved using the following proportionality constant:
    \begin{align}
        \NN(x\mid a, A) \NN(x\mid b, B) = \frac{Z_AZ_B}{Z_C} \exp\cb{-\frac{1}{2}\pn{a^TA\inv a \,+\, b^TB\inv b \,-\, c^T C\inv c}} \NN(x\mid c, C).
        \label{eq:norm-prod}
    \end{align}
    \label{thm:norm-prod}
\end{proposition}
\begin{proof}
The proportionality result is standard, so we leave that proof to the reader and use the result to derive our proportionality constant. Let $\norm{x-a}^2_A = (x - a)A\inv (x - a)^T$ be the squared Mahalanobis distance between $x$ and $a$ under covariance $A$, so that $\NN(x\mid a, A) = Z_{A}\exp(-\frac{1}{2}\norm{x-a}^2_A)$. The normalizing constant may therefore be written
\begin{align}
    \frac{\NN(x\mid a, A)\NN(x\mid b, B)}{\NN(x\mid c, C)} &= \frac{Z_AZ_B}{Z_C} \exp\cb{-\frac{1}{2}\pn{\norm{x - a}^2_A + \norm{x - b}^2_B - \norm{x - c}^2_C}}
\end{align}
Expanding the Mahalanobis distances, we see that the remaining terms are precisely those which to not vary in $x$ (which is natural given the proportionality result) yielding the desired result.
\end{proof}


The terms $K_{y^t_n, z, \theta_{C, n}}$ and $s(x;\, y^t_n, z, \theta^*)$ for the Gaussian mixture pose space model may be calculated using Proposition \ref{thm:norm-prod} by inverting the affine transform of $x$ in Eq. \ref{eq:norm-F-prod} and applying it to $y_n^t$, as in \ref{eq:cn-swap}. The mean and covariances of the constituent normals are then given in the language of the proposition by $a = C_n\inv (y + d_n)$, $A = C_n\inv R\, {C_n^T}\inv$, $b = m_z$, and $B = Q_z$, with dependence of $C_n, d_n$ on $\phi^*$ suppressed for readability, leaving $s$ to be the normal PDF $N(x\mid c, C)$ and $K$ to be the proportionality constant in Eq. \ref{eq:norm-prod}.


\subsubsection{Expectations of log probabilities}

It remains to be seen that $s$ can be analytically integrated against the requisite log probability terms: the quadratic form from Eq. \ref{eq:cn-swap} and $\log F$, which is also a quadratic form here. To calculate these integrals, we will use the following proposition.
\begin{proposition}
\label{prop:norm-quad-expect-2}
The expectation of a quadratic form in a normal variable is the sum of a Mahalanobis distance and a trace:
\begin{align}
    \EE_{x\sim\NN(a, B)} \bc{(x - c)^T D\inv (x - c)} = \norm{a - c}^2_D + \Tr\bc{BD\inv}
\end{align}
\end{proposition}
The proof is left to the reader, but is achieved by wrapping the whole expectation in a trace and cycling its arguments to arrive at an outer product $(x - c)(x - c)^T$. We may now enumerate the log probability terms that are to be calculated by the Gaussian mixture pose space model:
\begin{itemize}
    \item The expectation in $s$ of the quadratic form from Eq. \ref{eq:cn-swap} is given by the proposition, scaled by $-\frac{1}{2}$
    \item The expectation in $s$ of $\log F$ splits into two terms. First, we have the expectation of the quadratic form $-\frac{1}{2}\norm{x - m_z}^2_{Q_z}$, which is again given by the scaled result of the composition. Additionally, there is a term $\log Z_{Q_z} \propto -\frac{1}{2}\log\abs{Q_z}$ which is constant in $x$ and so is its own expectation.
\end{itemize}

\textbf{Priors}\quad We impose a heirarchical Dirichlet prior on the component weights $\pi_n$, but we do not constrain the parameters of the component distributions $m_z, Q_z$. This results in the following log-prior for the Gaussian mixture pose model:
\begin{align}
    L_{\text{morph}}(\gamma) =& \sum_n \log\text{Dir}(\pi_n \mid \hat\pi\;\beta) + \log\text{Dir}(\beta\mid \hat\beta / L)
\end{align}
Note that the hyperparameters $\hat\beta$, $\hat\pi$ define the variation tolerable in the expected weight of each component and the variation tolerable from that mean respectively. In particular, $\EE[\beta]$ is the discrete uniform over $L$ choices, with $\text{Var}[\beta_l]$ inversely proportional to $\hat\beta + 1$. Moreover, $\EE[\pi_n] = \beta$ with $\text{Var}[\pi_{n, l}]$ inversely proportional to $\hat\pi + 1$.

\textbf{Initialization}\quad We initialize the Gaussian mixture pose space model based on a standard Gaussian mixture model (GMM) fit to pose space data from a reference subject $\hat{n}$. Specifically, given parameters for a morph model, $\phi$, we form the dataset of pose space points $\cb{C_{\hat{n}}(\phi) y^t_n + d_{\hat{n}}(\phi)}_t$ and fit a GMM in $L$ components, yielding means $\hat{m}_z$, covariances $\hat{Q}_z$ and cluster weights $\hat\pi_z$. From these we may construct initialization parameters $\pi$ and $\psi$ as follows:
\begin{itemize}
    \item Component means and covariances of the pose space model are directly inherited from the reference-subject GMM. That is, $m_z = \hat{m}_z$ and $Q_z = \hat{Q}_z$.
    \item The heirarchical Dirichlet prior is initialized so that $\pi_{n, z} = \hat{\pi}_z$ and so that the initial $\pi_n$'s are the mean of their generating distribution, i.e. we take the component weight logits as $\bar\pi_{n, z} = \log \hat\pi_z$, and we take $\bar\beta_z = \log \hat\pi_z$ so that $\EE[\text{Dir}(\hat\pi \beta)] = \pi_n$.
\end{itemize}



\end{document}